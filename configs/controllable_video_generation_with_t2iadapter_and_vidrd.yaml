hydra:
  output_subdir: null
  run:
    dir: .

VDM:
  VidRD:
    _target_: models.ReuseAndDiffuse.model.SDVideoModel
    pretrained_model_path: "./checkpoints/stable-diffusion-2-1-base/"
    ckpt_path: "./checkpoints/VidRD/ModelT2V.pth"
    guidance_scale: 7.5
    num_inference_steps: 50
    resolution: 256
    add_temp_embed: true
    add_temp_conv: true

IDM:
  T2IAdapter:
    condition: "depth_midas" # ["depth_midas", "canny"]
    mixing_ratio: 1.0

MixedInversion:
  idm: "SD-v15"
  pretrained_model_path: "./checkpoints/stable-diffusion-v1-5"
  # mixing_ratio: 1.0 for ControlNet and InstructPix2Pix, 0.25 for Prompt2Prompt, 0.1 for StableDiffusion-Inpainting


Model:
  idm: "T2IAdapter"
  vdm: "VidRD"
  idm_prompt : "A red car moves in front of buildings"
  vdm_prompt : "A red car moves in front of buildings"
  video_path: "./data/car-turn-2.mp4"
  output_path: "./outputs/"
  video_length: 8
  width: 512
  height: 512
  frame_rate: 4
  seed: 42
  guidance_scale: 7.5
  num_inference_steps: 50

